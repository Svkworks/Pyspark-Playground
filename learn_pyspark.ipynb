{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307be84a-ae2f-43a4-b1f9-ee6a4505ce83",
   "metadata": {},
   "source": [
    "## APACHE SPARK / PYSPARK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4f08d-441c-405c-a495-4656806cea82",
   "metadata": {},
   "source": [
    "### What is Spark ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101658e-cb42-4f7f-b288-3b5b1e174ed0",
   "metadata": {},
   "source": [
    "Spark is in-memory distributed parallel processing engine with fault tolerant nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7e2f6-d8bc-4607-86ee-f613aa008179",
   "metadata": {},
   "source": [
    "### Architecture of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bbdad-ee3f-4a0e-bedb-928e8e4c4fef",
   "metadata": {},
   "source": [
    "<img src=\"https://aws.github.io/aws-emr-best-practices/assets/images/spark-bp-4-0ac9c7cc0bb2cff70ac01efa69455604.png\" width=\"600\"  height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ff19d-1ba8-4a78-a0cf-4fe685c2b458",
   "metadata": {},
   "source": [
    "**Driver:** \n",
    "1. Driver is heart of spark application\n",
    "2. Driver maintains the information of Worker node and it's executors\n",
    "3. Driver node analyse, distrubute and schedules the tasks to worker node\n",
    "4. This is the process the runs the main() function of spark application and that creates the SparkSession.\n",
    "\n",
    "-----\n",
    "\n",
    "**Executor:**\n",
    "1. It is a process that lunches on worker node in a spark applicaiton, and it runs the tasks and keeps the data in memory and disk.\n",
    "2. It responds to the driver with execution status\n",
    "\n",
    "------\n",
    "\n",
    "**cluster manager:**\n",
    "1. Cluster manager is responsible for allocating the resources to worker nodes such as number of executors,cores and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3ddef-d5d9-477e-99e8-5e65e632f3f6",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "**Partitions:**\n",
    "1. Partitions are nothing but the logical divisons of the data called partitions\n",
    "2. Each partition of the data will be processed by a signle task/core in Executor\n",
    "3. Partitions are important interms of maintain the parellelism and optimize the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637bc4b-f809-4281-9818-4404f37c33c1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Transformations:**\n",
    "1. Transformation is a process that accepts an RDD/DataFrame as an input and applying the logics on it and returns a RDD/Dataframe as an output\n",
    "2. This follows the lazy evaluation, that means the transformations will not be triggered immidetaly instead that waits until the  action is called.\n",
    "3. Lazy evalution is important to optimize the execution plan of spak.\n",
    "\n",
    "Example: **Select, Where, Group by, Order By, Limit**\n",
    "\n",
    "There are 2 types of transformations are available in spack ecosystem \n",
    "1. **Narrow Transformations:** In the narrow transformations, One input partitions produces only one output partition\n",
    "2. **Wide Transformations:** In the Wide transformations, multiple input partitions are needed to produces one output partitons, this step requires the shuffling the data.\n",
    "\n",
    "\n",
    "Narrow Transformation example: Select, Where, map() \\\n",
    "Wide Transformation example: Join, Group By, Order By, groupByKey(), reduceByKey(), sortByKey(), join(), cogroup()\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3334e-e451-489f-a522-781b26e9d80e",
   "metadata": {},
   "source": [
    "**Actions:**\n",
    "1. Action is a process that triggers the logical plan of tranformations and this will either returns the data back to customer or write the data to output data system.\n",
    "\n",
    "Example: Collect(), head(), tail(), show(), take(), Save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6671b-4ac2-4afb-8675-bfb36aa542dc",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ee449-dddb-4fff-846c-d4be1594cfac",
   "metadata": {},
   "source": [
    "#### RDD VS DATAFRAME VS DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160654a-f952-4685-9b81-29898c521cd5",
   "metadata": {},
   "source": [
    "<img src=\"https://media.licdn.com/dms/image/v2/C4D12AQHqtDQj79malQ/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1520122178712?e=2147483647&v=beta&t=gluvrQfDOKCjDbCHgBXi_ZMUY8H6oAJ1m0ghw-lNbY4\" width=\"500\"  height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b758a98-7edb-4037-9d77-d82e65596c94",
   "metadata": {},
   "source": [
    "**RDD:**\n",
    "1. The abbreviation of RDD is Resilient Distributed Datasets. It is an immutable collection of data objects that is distributed across the cluster and processed in parallel with fault-tolerant capabilities. \n",
    "2. It is a low-level API.\n",
    "3. It does not provide schema enforcement or optimization techniques.\n",
    "4. It is best suited for unstructured data and low-level transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33559f3f-8dd4-4918-8010-a0232a30e8f2",
   "metadata": {},
   "source": [
    "**DataFrames:**\n",
    "1. DataFrames are distributed collections of data organized into rows and columns, similar to a table in a relational database.\n",
    "2.\tThey are conceptually equivalent to relational tables.\n",
    "3.\tDataFrames provide a high-level API for data manipulation.\n",
    "4.\tThey are not type-safe, meaning if you try to access a column that doesn‚Äôt exist, the error will not be caught at compile time‚Äîit will be detected only at runtime.\n",
    "5.\tDataFrames use the Catalyst Optimizer for query optimization and code generation.\n",
    "6.\tSchema enforcement is supported, but only at runtime.\n",
    "7.\tThey support multiple programming languages, including Python, Scala, Java, and R.\n",
    "8.\tIdeal for working with structured and semi-structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b494ee-3a7b-46c4-901e-8d0d34b00b1b",
   "metadata": {},
   "source": [
    "**Dataset:**\n",
    "A Dataset is a distributed collection of data that combines the benefits of strong typing, compile-time type safety, and object-oriented programming. It can be thought of as a typed version of a DataFrame, where each row is represented as an object of a specific type, defined using a case class in Scala or a Java bean in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b8061-7d0b-492b-a6fa-3e623449672d",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ccfb35-9314-44d0-8073-6a9533c33e88",
   "metadata": {},
   "source": [
    "#### How spark works on partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0ff07-7834-4d39-9a76-56866b1fe4e9",
   "metadata": {},
   "source": [
    "<img src=\"https://images.squarespace-cdn.com/content/v1/58a3db0903596e0fdbb27391/1502484724596-T4IEBESUF806VN5CZOEZ/image-asset.png\" width=\"500\"  height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242fee1-e1a6-4a60-bb41-683b14c40fd5",
   "metadata": {},
   "source": [
    "1. When spark reads the data, then it automatically divides the data into partitions, often based on the data source i.e., Files in HDFS, or based on the User defined configurations.\n",
    "2. Each partition is a logical dataset of the data contains the subset of overall data\n",
    "3. Number of partitions can be controlled by User, the recommended to have the number of partitions that is a multiple of the number of cores in the cluster.\n",
    "4. Each partition is processed by a Task, which is a smallest unit of Spark Job.\n",
    "5. Spark assigns one task per partition, this enables spark to perform the parallel processing.\n",
    "6. When Spark applies transformations (like map, filter, join) to the data, it operates on partitions.\n",
    "7. Narrow transformations (like map, filter) can be applied within a partition without needing data from other partitions.\n",
    "8. Wide transformations (like groupByKey, join) might require data from multiple partitions to be shuffled and redistributed, potentially leading to a performance bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91734494-b090-4add-9098-c200aa3a7bc0",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c5a6a-b892-4a61-b018-667eae9dad10",
   "metadata": {},
   "source": [
    "### SPARK EXECUTION PLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef05817-c304-4012-b00f-bf05cb6f4d08",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4A-H9kJwnmbr9ivkBSazQ.png\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6723827-d2ce-48a1-9c93-38d3fbda9819",
   "metadata": {},
   "source": [
    "What Happens When You Submit a Spark Job (Spark 3 Flow)\n",
    "\n",
    "1. üßë‚Äçüíª User submits a job \\\n",
    "Code: spark-submit or runs a notebook/cell \\\n",
    "Includes transformations (map, filter) and actions (collect, count)\n",
    "\n",
    "\n",
    "3. üß† Unresolved Logical Plan is created \\\n",
    "Spark parses your code and builds a \"unresolved logical plan\" where relations and column names aren't specifically resolved\n",
    "This is a blueprint of what you want to do, not how to do it yet\n",
    "\n",
    "4. üß† Analyzed Logical Plan is created \\\n",
    "Unresolved logical plan then uses the Metadata catalog and validates the data structures, schema and types, and if everything goes well then this plan marked as \"analyzed logical plan\"\n",
    "\n",
    "5. üßπ Optimized Logical Plan \\\n",
    "Spark uses Catalyst Optimizer to improve the performance: \\\n",
    "Reorder operations\\\n",
    "Push down filters\\\n",
    "projection pruning\\\n",
    "Simplify expressions\\\n",
    "The result: an Optimized Logical Plan\n",
    "\n",
    "Also **Lineage (we can get this using Explain or toDebugString** \\\n",
    "   **Predicate pushdown:** Moving the filter condistions [predicates] tp closer to data sources, it means filtering logic as early as possible during data retrieval or processing, rather than loading the entire dataset into memory and then filtering it) \\\n",
    "   \n",
    "   **projection pruning:** Removing the unnessary columns while processing the query.\n",
    "\n",
    "5. ‚öôÔ∏è Physical Plan is created \\\n",
    "Spark translates the optimized Logical plan into a Physical Plan\\\n",
    "Now it decides how to execute:\\\n",
    "By considering the data partitions\\\n",
    "data shuffling\\\n",
    "task distributions accross the nodes\\\n",
    "sort-merge join vs broadcast join\n",
    "\n",
    "6. ‚úÖ Best Physical Plan is selected\\\n",
    "This Physical plans runs against cost based models,Which basically generate costs of each physical plans\\\n",
    "Spark compares multiple execution strategies and chooses the most efficient one that costs less\\\n",
    "\n",
    "7. üì¶ DAG (Directed Acyclic Graph) is built\\\n",
    "The physical plan is converted into a DAG of stages and tasks (Job, stages, and tasks)\\\n",
    "Each Stage = group of tasks that can be run together (usually after shuffle)\\\n",
    "\n",
    "8. üéØ Job is submitted to the cluster\\\n",
    "The Driver Program sends the DAG to the Cluster Manager (like YARN/K8s/Standalone)\\\n",
    "\n",
    "9. üß© Cluster Manager allocates Executors\\\n",
    "Executors are launched on worker nodes\\\n",
    "Each Executor gets a portion of your data to process\\\n",
    "\n",
    "10. üèÉ‚Äç‚ôÇÔ∏è Tasks are scheduled and run\\\n",
    "Spark breaks down the stages into Tasks (smallest units of work)\\\n",
    "Each task runs on an executor and processes a partition of data\\\n",
    "\n",
    "11. üì§ Results are collected or saved\\\n",
    "If action is collect() ‚Üí data is returned to the driver\\\n",
    "If write() ‚Üí data is saved to S3, HDFS, BigQuery, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06212f04-33e1-49d8-8f8a-aa256d60c159",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b154f37-3682-4e95-854d-167dd36e5ae4",
   "metadata": {},
   "source": [
    "### DAG (Directed Acyclic Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd193d-47a4-489e-a6cd-2b08c03c20d5",
   "metadata": {},
   "source": [
    "<img src=\"https://techvidvan.com/tutorials/wp-content/uploads/2019/11/DAG-Visualisation-01.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22189336-a794-4860-86bb-3aa694829de2",
   "metadata": {},
   "source": [
    "**Dag:** \n",
    "1. Dag represents the \"Optimized logical plan\" in the spark execution, it consists of transformations and actions.\n",
    "2. DagScheduler is responsible for converting these transformations and actions into DAG of Jobs and Job consists of stages and tasks, which can be executed parallel in the cluster\n",
    "3. **Stage:** It is group of tasks called a stage, and these tasks can be executed parallel. There are 2 types of stages are available in Dag i.e., Shuffle stage and non-shuffle stages.\n",
    "4. **Tasks:** Task is a single unit of work that will be executed on single partition in spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed5b57-a8a3-4610-baa9-eb85a82b4e45",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb5ef9-feaf-4e9b-8947-8f5b7e0f23b2",
   "metadata": {},
   "source": [
    "## Introduction to SparkSession and Coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33eec694-da7e-4993-b980-43e624be6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"test\")\n",
    "         .config(\"spark.executor.instances\", 4)\n",
    "         .config(\"spark.executor.cores\", 4)\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1adeac2f-6321-431c-9c9a-da6463d0fb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x141358350>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is SparkSession \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "494e60c8-676f-4fda-8457-4d0fc93b3741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', IntegerType(), True), StructField('age', IntegerType(), True), StructField('gender', StringType(), True), StructField('occupation', StringType(), True), StructField('zipcode', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#Create scheam for user table\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, _parse_datatype_string\n",
    "\n",
    "user_schema = StructType([\n",
    "StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "StructField(name=\"age\", dataType=IntegerType(), nullable=True),\n",
    "StructField(name=\"gender\", dataType=StringType(), nullable=True),\n",
    "StructField(name=\"occupation\", dataType=StringType(), nullable=True),\n",
    "StructField(name=\"zipcode\", dataType=IntegerType(), nullable=True)\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# Create a schema using schema string using _parse_datatype_string\n",
    "schema_str = \"id int, age int, gender string, occupation string, zipcode int\"\n",
    "user_parsed_schema = _parse_datatype_string(schema_str)\n",
    "print(user_parsed_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f7d40-8dda-4022-ae48-2f3092e5c743",
   "metadata": {},
   "source": [
    "#### Creation of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75c29390-0d76-4f7e-8536-2a66bb837ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.read.csv(path=\"data/u.user\", sep=\"|\", schema=user_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46ad2f-2abe-46c7-b061-c2433453b19b",
   "metadata": {},
   "source": [
    "#### check Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef70e65e-b79c-459d-b835-3ca2f5d84072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+\n",
      "| id|age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|  1| 24|     M|technician|  85711|\n",
      "|  2| 53|     F|     other|  94043|\n",
      "+---+---+------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1dfea-ec02-492f-804d-62ae39d7a70a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bd1e3-ba8d-417f-a9ad-e31dc3be3b90",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcb610-bb80-4e57-ba52-cd60aca67ae4",
   "metadata": {},
   "source": [
    "#### View Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d4bbf55-8faa-475c-8eb9-8a185212ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da59b83-9dd5-44a1-89fb-0e44dcfacecd",
   "metadata": {},
   "source": [
    "#### selecting the values on DF\n",
    "There are different ways of selecting the columns in spark \\\n",
    "1.col and expr \\\n",
    "2.df[\"col_name\"] \\\n",
    "3.\"col_name\" \\\n",
    "4.df.col_name \\\n",
    "5.selectExpr(\"col_name1\", \"col_name2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b72a3de5-0e78-43b9-9b9e-69699d21aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+\n",
      "|id |new_id|age|gender|occupation|\n",
      "+---+------+---+------+----------+\n",
      "|1  |1     |24 |M     |technician|\n",
      "|2  |2     |53 |F     |other     |\n",
      "|3  |3     |23 |M     |writer    |\n",
      "|4  |4     |24 |M     |technician|\n",
      "|5  |5     |33 |F     |other     |\n",
      "+---+------+---+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+------+---+------+----------+\n",
      "|id |new_id|age|gender|occupation|\n",
      "+---+------+---+------+----------+\n",
      "|1  |1     |24 |M     |technician|\n",
      "|2  |2     |53 |F     |other     |\n",
      "+---+------+---+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "user_df.select(col(\"id\"), expr(\"id as new_id\"), user_df[\"age\"], \"gender\", user_df.occupation).show(5, truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "user_df.selectExpr(\"id\",\"id as new_id\", \"age\", \"gender\", \"occupation\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34adb4-e700-43d6-b6c3-80be6f059c51",
   "metadata": {},
   "source": [
    "#### Filtering the values on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcb8847f-e5f2-4348-9436-47ad0dcae8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+\n",
      "|id |age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|44 |26 |M     |technician|46260  |\n",
      "|178|26 |M     |other     |49512  |\n",
      "|440|30 |M     |other     |48076  |\n",
      "|689|25 |M     |other     |45439  |\n",
      "+---+---+------+----------+-------+\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+---+------+----------+-------+\n",
      "|id |age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|44 |26 |M     |technician|46260  |\n",
      "|178|26 |M     |other     |49512  |\n",
      "|440|30 |M     |other     |48076  |\n",
      "|689|25 |M     |other     |45439  |\n",
      "+---+---+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.where(\n",
    "    (col(\"id\") > 1)\n",
    "    & (user_df[\"age\"].between(25, 35))\n",
    "    & (col(\"gender\") == \"M\")\n",
    "    & (user_df.occupation.isin(\"other\", \"technician\"))\n",
    "    & (user_df.zipcode.cast(StringType()).ilike(\"4%\"))\n",
    ")\\\n",
    ".show(5, truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "user_df.where(\"\"\"\n",
    "(id > 1)\n",
    "AND (age BETWEEN 25 AND 35)\n",
    "AND (gender = 'M')\n",
    "AND (occupation in ('other', 'technician'))\n",
    "AND (cast(zipcode as string) LIKE ('4%'))\n",
    "\"\"\")\\\n",
    ".show(5, truncate=False)              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb445c-e769-4e4e-a6a2-aeb1cfbb2f7d",
   "metadata": {},
   "source": [
    "#### CAST Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "994140c9-a6ca-4070-a5e4-6b0546b16613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+\n",
      "|string_id|string_age|zipcode|\n",
      "+---------+----------+-------+\n",
      "|1        |24        |85711  |\n",
      "|2        |53        |94043  |\n",
      "|3        |23        |32067  |\n",
      "+---------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.select(col('id').cast('string').alias('string_id'), \\\n",
    "               col('age').cast(StringType()).alias('string_age'), \\\n",
    "               col('zipcode').cast('integer')\n",
    "              ) \\\n",
    ".show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66789e7-9864-42c8-8c39-61635c49b835",
   "metadata": {},
   "source": [
    "#### Adding a new column/columns and CASE WHEN STATEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ce1e321-d956-414c-a2d8-fc32a33762cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+----------+\n",
      "|id |age|gender|occupation|zipcode|age_group |\n",
      "+---+---+------+----------+-------+----------+\n",
      "|1  |24 |M     |technician|85711  |90's KID  |\n",
      "|2  |53 |F     |other     |94043  |Old Person|\n",
      "|3  |23 |M     |writer    |32067  |90's KID  |\n",
      "|4  |24 |M     |technician|43537  |90's KID  |\n",
      "|5  |33 |F     |other     |15213  |80's KID  |\n",
      "+---+---+------+----------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+---+------+----------+-------+----------+\n",
      "|id |age|gender|occupation|zipcode|age_group |\n",
      "+---+---+------+----------+-------+----------+\n",
      "|1  |24 |M     |technician|85711  |90's KID  |\n",
      "|2  |53 |F     |other     |94043  |Old Person|\n",
      "|3  |23 |M     |writer    |32067  |90's KID  |\n",
      "|4  |24 |M     |technician|43537  |90's KID  |\n",
      "|5  |33 |F     |other     |15213  |80's KID  |\n",
      "+---+---+------+----------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+---+------+----------+-------+----------+\n",
      "|id |age|gender|occupation|zipcode|age_group |\n",
      "+---+---+------+----------+-------+----------+\n",
      "|1  |24 |M     |technician|85711  |90's KID  |\n",
      "|2  |53 |F     |other     |94043  |Old Person|\n",
      "|3  |23 |M     |writer    |32067  |90's KID  |\n",
      "|4  |24 |M     |technician|43537  |90's KID  |\n",
      "|5  |33 |F     |other     |15213  |80's KID  |\n",
      "+---+---+------+----------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+---+------+----------+-------+-------+------------------+\n",
      "|id |age|gender|occupation|zipcode|new_age|gender_abbrivation|\n",
      "+---+---+------+----------+-------+-------+------------------+\n",
      "|1  |24 |M     |technician|85711  |9.6    |Male              |\n",
      "|2  |53 |F     |other     |94043  |21.2   |Female            |\n",
      "|3  |23 |M     |writer    |32067  |9.2    |Male              |\n",
      "|4  |24 |M     |technician|43537  |9.6    |Male              |\n",
      "|5  |33 |F     |other     |15213  |13.2   |Female            |\n",
      "+---+---+------+----------+-------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "user_df.withColumn(\"age_group\", \n",
    "                  f.when(col(\"age\") <= 20, \"20's KID\") \\\n",
    "                   .when(col(\"age\").between(20,30), \"90's KID\") \\\n",
    "                   .when(col(\"age\").between(30,40) , \"80's KID\") \\\n",
    "                   .otherwise(\"Old Person\")\n",
    "                  ).show(5, False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "user_df.select(\"*\", \n",
    "                  f.when(col(\"age\") <= 20, \"20's KID\") \\\n",
    "                   .when(col(\"age\").between(20,30), \"90's KID\") \\\n",
    "                   .when(col(\"age\").between(30,40) , \"80's KID\") \\\n",
    "                   .otherwise(\"Old Person\").alias(\"age_group\")\n",
    "                  ).show(5, False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "user_df.select(\"*\", expr(\n",
    "    \"\"\"\n",
    "    CASE WHEN age <= 20 THEN \"20\\'s KID\"\n",
    "        WHEN age BETWEEN 20 AND 30 THEN \"90\\'s KID\"\n",
    "        WHEN age BETWEEN 30 AND 40 THEN \"80\\'s KID\"\n",
    "        ELSE \"Old Person\" END AS age_group\n",
    "    \"\"\"\n",
    ")).show(5, truncate=False)\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "# Adding multiple columns \n",
    "\n",
    "columns = {\n",
    "    \"new_age\" : f.round(col(\"age\") * 0.4, 2),\n",
    "    \"gender_abbrivation\" : f.when(col(\"gender\") == \"M\", \"Male\").otherwise(\"Female\") \n",
    "}\n",
    "\n",
    "user_df.withColumns(columns).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2021b-2b46-479a-b249-ca1985022da3",
   "metadata": {},
   "source": [
    "#### Adding the Literals (lit & typeLit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11b5fca1-4899-4a65-90cb-1c72ff1ca37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+------------------+\n",
      "|id |age|gender|occupation|zipcode|gender_abbrevation|\n",
      "+---+---+------+----------+-------+------------------+\n",
      "|1  |24 |M     |technician|85711  |Male              |\n",
      "|2  |53 |F     |other     |94043  |Female            |\n",
      "|3  |23 |M     |writer    |32067  |Male              |\n",
      "|4  |24 |M     |technician|43537  |Male              |\n",
      "|5  |33 |F     |other     |15213  |Female            |\n",
      "+---+---+------+----------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+---+------+----------+-------+-------------------------------+\n",
      "|id |age|gender|occupation|zipcode|gender_facilites               |\n",
      "+---+---+------+----------+-------+-------------------------------+\n",
      "|1  |24 |M     |technician|85711  |[job notifcation, scholerships]|\n",
      "|2  |53 |F     |other     |94043  |[free bus, protection]         |\n",
      "|3  |23 |M     |writer    |32067  |[job notifcation, scholerships]|\n",
      "|4  |24 |M     |technician|43537  |[job notifcation, scholerships]|\n",
      "|5  |33 |F     |other     |15213  |[free bus, protection]         |\n",
      "+---+---+------+----------+-------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# non sequence literal value\n",
    "user_df.withColumn(\"gender_abbrevation\", f.when(col(\"gender\") == \"F\", f.lit(\"Female\")).otherwise(lit(\"Male\")))\\\n",
    ".show(5, False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "# sequence literal value\n",
    "user_df.withColumn(\"gender_facilites\", \n",
    "                   f.when(col(\"gender\") == \"F\", f.lit([\"free bus\", \"protection\"]))\\\n",
    "                   .otherwise(lit([\"job notifcation\", \"scholerships\"])))\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63857e1c-8ae6-435a-b6a7-a1395e8394db",
   "metadata": {},
   "source": [
    "#### Renaming the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c2f8667-9fb3-4072-9c93-8985b0ac608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-------+\n",
      "|user_id|age|gender|occupation|zipcode|\n",
      "+-------+---+------+----------+-------+\n",
      "|1      |24 |M     |technician|85711  |\n",
      "|2      |53 |F     |other     |94043  |\n",
      "|3      |23 |M     |writer    |32067  |\n",
      "|4      |24 |M     |technician|43537  |\n",
      "|5      |33 |F     |other     |15213  |\n",
      "+-------+---+------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+-------+-------+------+----------+-------+\n",
      "|user_id|new_age|gender|occupation|zipcode|\n",
      "+-------+-------+------+----------+-------+\n",
      "|1      |24     |M     |technician|85711  |\n",
      "|2      |53     |F     |other     |94043  |\n",
      "|3      |23     |M     |writer    |32067  |\n",
      "|4      |24     |M     |technician|43537  |\n",
      "|5      |33     |F     |other     |15213  |\n",
      "+-------+-------+------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Single Column Renamed\n",
    "user_df.withColumnRenamed(\"id\", \"user_id\").show(5, False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "# Multiple Column Renamed\n",
    "user_df.withColumnsRenamed({\"id\": \"user_id\", \"age\" : \"new_age\"}).show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede0751-ec29-417b-acb2-ce4b74774cad",
   "metadata": {},
   "source": [
    "#### Ordering the data i.e., ASC, DESC, NULLS FIRST and NULLS LAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d004405f-7903-45cb-9753-19cb86b8b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------+\n",
      "|id |age |gender|occupation|zipcode|\n",
      "+---+----+------+----------+-------+\n",
      "|116|NULL|M     |healthcare|97232  |\n",
      "|17 |NULL|M     |programmer|6355   |\n",
      "|30 |7   |M     |student   |55436  |\n",
      "|471|10  |M     |student   |77459  |\n",
      "|289|11  |M     |none      |94619  |\n",
      "+---+----+------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "+---+----+------+----------+-------+\n",
      "|id |age |gender|occupation|zipcode|\n",
      "+---+----+------+----------+-------+\n",
      "|116|NULL|M     |healthcare|97232  |\n",
      "|17 |NULL|M     |programmer|6355   |\n",
      "|30 |7   |M     |student   |55436  |\n",
      "|471|10  |M     |student   |77459  |\n",
      "|289|11  |M     |none      |94619  |\n",
      "+---+----+------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.orderBy(col(\"age\").asc(),\n",
    "               col(\"zipcode\").desc(),\n",
    "               col(\"id\").asc_nulls_first(),\n",
    "               col(\"occupation\").desc_nulls_last()\n",
    "               ).show(5, False)\n",
    "\n",
    "print(\"\"\"\n",
    "=============================\n",
    "\"\"\")\n",
    "\n",
    "user_df.sort(col(\"age\").asc(),\n",
    "               col(\"zipcode\").desc(),\n",
    "               col(\"id\").asc_nulls_first(),\n",
    "               col(\"occupation\").desc_nulls_last()\n",
    "               ).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2ca24-1ec6-4e2e-993a-bbaf9bee5c45",
   "metadata": {},
   "source": [
    "#### String functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "143b382f-ca18-436e-8a4c-8f3d4afb1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+-----------------+\n",
      "|id |age|gender|occupation|zipcode|len_of_occupation|\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "|1  |24 |M     |technician|85711  |10               |\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "|id |age|gender|occupation|zipcode|len_of_occupation|\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "|1  |24 |M     |technician|85711  |techn            |\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "|id |age|gender|occupation|zipcode|len_of_occupation|\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "|1  |24 |M     |technician|85711  |technicizn       |\n",
      "+---+---+------+----------+-------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.withColumn('len_of_occupation', f.length(user_df['occupation'])).show(1, False)\n",
    "print(\"\"\"=============================\"\"\")\n",
    "user_df.withColumn('len_of_occupation', f.substring(user_df['occupation'], 1, 5)).show(1, False)\n",
    "print(\"\"\"=============================\"\"\")\n",
    "user_df.withColumn('len_of_occupation', f.regexp_replace(user_df['occupation'], 'a', 'z')).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694eaf39-3853-407f-b6de-73be0a4b0b81",
   "metadata": {},
   "source": [
    "#### Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e01e5b7-7c92-4532-bc5f-f455c9ec4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+----------+\n",
      "|id |age|gender|occupation|zipcode|dob       |\n",
      "+---+---+------+----------+-------+----------+\n",
      "|1  |24 |M     |technician|85711  |2025-01-13|\n",
      "|2  |53 |F     |other     |94043  |2025-01-13|\n",
      "|3  |23 |M     |writer    |32067  |2025-01-13|\n",
      "+---+---+------+----------+-------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+------------+\n",
      "|id |age|gender|occupation|zipcode|current_date|\n",
      "+---+---+------+----------+-------+------------+\n",
      "|1  |24 |M     |technician|85711  |2025-09-13  |\n",
      "|2  |53 |F     |other     |94043  |2025-09-13  |\n",
      "|3  |23 |M     |writer    |32067  |2025-09-13  |\n",
      "+---+---+------+----------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+-----------+\n",
      "|id |age|gender|occupation|zipcode|todays_date|\n",
      "+---+---+------+----------+-------+-----------+\n",
      "|1  |24 |M     |technician|85711  |13/09/2025 |\n",
      "|2  |53 |F     |other     |94043  |13/09/2025 |\n",
      "|3  |23 |M     |writer    |32067  |13/09/2025 |\n",
      "+---+---+------+----------+-------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.withColumn(\"dob\", f.to_date(lit(\"2025-13-01\"), \"yyyy-dd-MM\")).show(3, False)\n",
    "print(\"\"\"=============================\"\"\")\n",
    "user_df.withColumn(\"current_date\", f.current_date()).show(3, False)\n",
    "print(\"\"\"=============================\"\"\")\n",
    "\n",
    "# convert the date into String\n",
    "user_df.withColumn(\"todays_date\", f.date_format(f.current_date(), 'dd/MM/yyyy')).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39e08-9cea-4fd2-ac31-a7223a44e3f7",
   "metadata": {},
   "source": [
    "#### Drop Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10f59f72-ed2d-4f81-a0f8-ce6a1eb6af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+\n",
      "|id |age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|1  |24 |M     |technician|85711  |\n",
      "|2  |53 |F     |other     |94043  |\n",
      "+---+---+------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, age: int, gender: string, occupation: string, zipcode: int]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Nulls from all columns\n",
    "user_df.na.drop() # user_df.drop_duplicates()\n",
    "\n",
    "# Drop Nulls from specific column\n",
    "user_df.na.drop(subset=[\"age\", \"occupation\"]) # user_df.drop_duplicates(subset=[\"age\", \"occupation\"])\n",
    "\n",
    "# drop column \n",
    "user_df.withColumn(\"current_date\" , lit(f.current_date())).drop(\"current_date\").show(2, False)\n",
    "\n",
    "# Drop duplicates\n",
    "user_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b676e9-caaf-40e4-9997-b86b1c4d5686",
   "metadata": {},
   "source": [
    "#### Union Vs Union ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21323e5e-91e9-4391-9119-45393a440f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+\n",
      "|id |age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|1  |24 |M     |technician|85711  |\n",
      "|2  |53 |F     |other     |94043  |\n",
      "|3  |23 |M     |writer    |32067  |\n",
      "|4  |24 |M     |technician|43537  |\n",
      "|5  |33 |F     |other     |15213  |\n",
      "|6  |22 |M     |Software  |89893  |\n",
      "|1  |24 |M     |technician|85711  |\n",
      "+---+---+------+----------+-------+\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+\n",
      "|id |age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|3  |23 |M     |writer    |32067  |\n",
      "|1  |24 |M     |technician|85711  |\n",
      "|4  |24 |M     |technician|43537  |\n",
      "|5  |33 |F     |other     |15213  |\n",
      "|2  |53 |F     |other     |94043  |\n",
      "|6  |22 |M     |Software  |89893  |\n",
      "+---+---+------+----------+-------+\n",
      "\n",
      "=============================\n",
      "+---+---+------+----------+-------+\n",
      "| id|age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|  1| 24|     M|technician|  85711|\n",
      "|  2| 53|     F|     other|  94043|\n",
      "|  3| 23|     M|    writer|  32067|\n",
      "|  4| 24|     M|technician|  43537|\n",
      "|  5| 33|     F|     other|  15213|\n",
      "|  6| 22|     M|  Software|  89893|\n",
      "|  1| 24|     M|technician|  85711|\n",
      "+---+---+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df_1 = user_df.limit(5)\n",
    "\n",
    "emp_data = [[6,22,'M', 'Software', 89893], [1,24,'M', 'technician', 85711]]\n",
    "emp_df_2 = spark.createDataFrame(data=emp_data, schema=user_df.columns)\n",
    "\n",
    "# Union all example\n",
    "emp_df_1.union(emp_df_2).show(7, False)\n",
    "\n",
    "print(\"\"\"=============================\"\"\")\n",
    "\n",
    "# Union distinct example\n",
    "emp_df_1.union(emp_df_2).distinct().show(7, False)\n",
    "\n",
    "print(\"\"\"=============================\"\"\")\n",
    "\n",
    "# Union by Name\n",
    "emp_df_1.unionByName(emp_df_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efaae9-1c6f-429e-a735-41d8fa936bab",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "759a75f7-e381-4d79-ba5d-32be2c1308b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|avg_age_per_gender|\n",
      "+------+------------------+\n",
      "|     F|             33.83|\n",
      "|     M|             34.15|\n",
      "|  NULL|              28.0|\n",
      "+------+------------------+\n",
      "\n",
      "+-------------+-------------------+\n",
      "|occupation   |count_of_occupation|\n",
      "+-------------+-------------------+\n",
      "|student      |196                |\n",
      "|other        |105                |\n",
      "|educator     |95                 |\n",
      "|administrator|78                 |\n",
      "|engineer     |66                 |\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "|  NULL|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avg age per Gender\n",
    "user_df.groupby(col(\"gender\"))\\\n",
    ".agg(f.round(f.avg(user_df.age),2).alias(\"avg_age_per_gender\"))\\\n",
    ".show()\n",
    "\n",
    "# count of occupations \n",
    "user_df.groupby(user_df.occupation)\\\n",
    ".agg(f.count(user_df.id).alias(\"count_of_occupation\"))\\\n",
    ".orderBy(col(\"count_of_occupation\").desc())\\\n",
    ".show(5, False)\n",
    "\n",
    "# distinct column values\n",
    "user_df.select(user_df.gender).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e782fd-1dfc-40c9-8476-6530c37a52ff",
   "metadata": {},
   "source": [
    "#### \n",
    "Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4554fccc-9581-4baf-98a1-cfd7e920006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+---+\n",
      "|id |age|gender|occupation|zipcode|rnk|\n",
      "+---+---+------+----------+-------+---+\n",
      "|609|13 |F     |student   |55106  |1  |\n",
      "|674|13 |F     |student   |55337  |1  |\n",
      "|206|14 |F     |student   |53115  |2  |\n",
      "|813|14 |F     |student   |2136   |2  |\n",
      "|887|14 |F     |student   |27249  |2  |\n",
      "+---+---+------+----------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+------+-------------+-------+-----------------+\n",
      "|id |age|gender|occupation   |zipcode|max_age_until_now|\n",
      "+---+---+------+-------------+-------+-----------------+\n",
      "|860|70 |F     |retired      |48322  |70.0             |\n",
      "|266|62 |F     |administrator|78756  |66.0             |\n",
      "|131|59 |F     |administrator|15237  |63.67            |\n",
      "|754|59 |F     |librarian    |62901  |62.5             |\n",
      "|591|57 |F     |librarian    |92093  |61.4             |\n",
      "+---+---+------+-------------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(col(\"gender\")).orderBy(col(\"age\"))\n",
    "\n",
    "# Dense Rank\n",
    "user_df.withColumn(\"rnk\", f.dense_rank().over(window=window)).show(5,False)\n",
    "\n",
    "# Max Age\n",
    "new_window = window.rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "user_df.withColumn(\"max_age_until_now\", \n",
    "                   f.round(f.avg(col(\"age\"))\n",
    "                   .over(window=Window.partitionBy(col(\"gender\"))\n",
    "                         .orderBy(f.desc(\"age\"))\n",
    "                         .rowsBetween(Window.unboundedPreceding, 0))\n",
    "                  ,2)).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce46b0-bbd8-4738-a1ed-2d0f31573041",
   "metadata": {},
   "source": [
    "#### Partitions \n",
    "1. **Repartition:** The repartition() can be used to increase or decrease the number of partitions, \n",
    "but it involves heavy data shuffling across the cluster.\n",
    "2. **coalesce:** coalesce() can be used only to decrease the number of partitions. In most of the cases, coalesce() does not trigger a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ae7f316-5e65-442a-aa03-ba2d25362519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "4\n",
      "+---+---+------+-------------+-------+------------+\n",
      "|id |age|gender|occupation   |zipcode|partition_id|\n",
      "+---+---+------+-------------+-------+------------+\n",
      "|63 |31 |M     |marketing    |75240  |2           |\n",
      "|91 |55 |M     |marketing    |1913   |2           |\n",
      "|95 |31 |M     |administrator|10707  |2           |\n",
      "|115|31 |M     |engineer     |17110  |2           |\n",
      "|131|59 |F     |administrator|15237  |2           |\n",
      "|134|31 |M     |programmer   |80236  |2           |\n",
      "|145|31 |M     |entertainment|NULL   |2           |\n",
      "|172|55 |M     |marketing    |22207  |2           |\n",
      "|197|55 |M     |technician   |75094  |2           |\n",
      "|224|31 |F     |educator     |43512  |2           |\n",
      "|269|31 |F     |librarian    |43201  |2           |\n",
      "|295|31 |M     |educator     |50325  |2           |\n",
      "|315|31 |M     |educator     |18301  |2           |\n",
      "|388|31 |M     |other        |36106  |2           |\n",
      "|413|55 |M     |educator     |78212  |2           |\n",
      "|418|55 |F     |none         |21206  |2           |\n",
      "|426|55 |M     |educator     |1602   |2           |\n",
      "|489|55 |M     |other        |45218  |2           |\n",
      "|538|31 |M     |scientist    |21010  |2           |\n",
      "|578|31 |M     |administrator|NULL   |2           |\n",
      "|593|31 |F     |educator     |68767  |2           |\n",
      "|616|55 |M     |scientist    |50613  |2           |\n",
      "|653|31 |M     |executive    |55105  |2           |\n",
      "|659|31 |M     |educator     |54248  |2           |\n",
      "|662|55 |M     |librarian    |19102  |2           |\n",
      "|687|31 |F     |healthcare   |27713  |2           |\n",
      "|696|55 |M     |other        |94920  |2           |\n",
      "|724|31 |M     |executive    |40243  |2           |\n",
      "|730|31 |F     |scientist    |32114  |2           |\n",
      "|743|31 |M     |programmer   |92660  |2           |\n",
      "+---+---+------+-------------+-------+------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get No of Partitions\n",
    "print(user_df.rdd.getNumPartitions())\n",
    "\n",
    "# Increase the partitions \n",
    "print(user_df.repartition(5).rdd.getNumPartitions())\n",
    "\n",
    "# Decrease the partitions \n",
    "print(user_df.repartition(4).rdd.getNumPartitions())\n",
    "\n",
    "# Identify the spark_partition_id value\n",
    "user_df.repartition(100, col(\"age\")).withColumn(\"partition_id\", f.spark_partition_id()).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402a102-956e-4f43-acd1-f2f090efb909",
   "metadata": {},
   "source": [
    "#### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "104e50b6-0886-4cea-a47c-de9777a5bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+--------+----------+\n",
      "| id|age|gender|occupation|zipcode|zip_code|      city|\n",
      "+---+---+------+----------+-------+--------+----------+\n",
      "|653| 31|     M| executive|  55105|   55105|Dusseldorf|\n",
      "|421| 38|     F|programmer|  55105|   55105|Dusseldorf|\n",
      "|352| 37|     F|programmer|  55105|   55105|Dusseldorf|\n",
      "|196| 49|     M|    writer|  55105|   55105|Dusseldorf|\n",
      "| 52| 18|     F|   student|  55105|   55105|Dusseldorf|\n",
      "+---+---+------+----------+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+------+----------+-------+--------+----+\n",
      "| id|age|gender|occupation|zipcode|zip_code|city|\n",
      "+---+---+------+----------+-------+--------+----+\n",
      "|  3| 23|     M|    writer|  32067|    NULL|NULL|\n",
      "|  5| 33|     F|     other|  15213|    NULL|NULL|\n",
      "|  4| 24|     M|technician|  43537|    NULL|NULL|\n",
      "|  1| 24|     M|technician|  85711|    NULL|NULL|\n",
      "|  6| 42|     M| executive|  98101|    NULL|NULL|\n",
      "+---+---+------+----------+-------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+------+----------+-------+--------+----------+\n",
      "| id|age|gender|occupation|zipcode|zip_code|      city|\n",
      "+---+---+------+----------+-------+--------+----------+\n",
      "|653| 31|     M| executive|  55105|   55105|Dusseldorf|\n",
      "|421| 38|     F|programmer|  55105|   55105|Dusseldorf|\n",
      "|352| 37|     F|programmer|  55105|   55105|Dusseldorf|\n",
      "|196| 49|     M|    writer|  55105|   55105|Dusseldorf|\n",
      "| 52| 18|     F|   student|  55105|   55105|Dusseldorf|\n",
      "| 37| 23|     M|   student|  55105|   55105|Dusseldorf|\n",
      "|687| 31|     F|healthcare|  27713|   27713|     Essen|\n",
      "|730| 31|     F| scientist|  32114|   32114|  Dortmund|\n",
      "|315| 31|     M|  educator|  18301|   18301|   Cologne|\n",
      "|538| 31|     M| scientist|  21010|   21010|    Berlin|\n",
      "+---+---+------+----------+-------+--------+----------+\n",
      "\n",
      "+---+---+------+-------------+-------+\n",
      "| id|age|gender|   occupation|zipcode|\n",
      "+---+---+------+-------------+-------+\n",
      "|131| 59|     F|administrator|  15237|\n",
      "|619| 17|     M|      student|  44134|\n",
      "|409| 48|     M|administrator|  98225|\n",
      "|839| 38|     F|entertainment|  90814|\n",
      "|271| 51|     M|     engineer|  22932|\n",
      "+---+---+------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+------+----------+-------+\n",
      "| id|age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|315| 31|     M|  educator|  18301|\n",
      "|538| 31|     M| scientist|  21010|\n",
      "|687| 31|     F|healthcare|  27713|\n",
      "|730| 31|     F| scientist|  32114|\n",
      "| 37| 23|     M|   student|  55105|\n",
      "| 52| 18|     F|   student|  55105|\n",
      "|196| 49|     M|    writer|  55105|\n",
      "|352| 37|     F|programmer|  55105|\n",
      "|421| 38|     F|programmer|  55105|\n",
      "|653| 31|     M| executive|  55105|\n",
      "+---+---+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [[55105, 'Dusseldorf'], [27713, 'Essen'], [32114, 'Dortmund'], [18301, 'Cologne'], [21010, 'Berlin']]\n",
    "dept_schema = ['zip_code', 'city']\n",
    "\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema=dept_schema)\n",
    "\n",
    "# Inner\n",
    "user_dept_df = user_df.join(dept_df, user_df.zipcode == dept_df.zip_code, how='inner').show(5)\n",
    "\n",
    "#left\n",
    "user_dept_df = user_df.join(dept_df, user_df.zipcode == dept_df.zip_code, how='left').show(5)\n",
    "\n",
    "#Right\n",
    "user_dept_df = user_df.join(dept_df, user_df.zipcode == dept_df.zip_code, how='right').show(100)\n",
    "\n",
    "#anti\n",
    "user_dept_df = user_df.join(dept_df, user_df.zipcode == dept_df.zip_code, how='anti').show(5)\n",
    "\n",
    "#anti\n",
    "user_dept_df = user_df.join(dept_df, user_df.zipcode == dept_df.zip_code, how='semi').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03eec38-251a-4eaf-b8d3-74a8edf584c4",
   "metadata": {},
   "source": [
    "#### Reading the CSV data\n",
    "\n",
    "We are focusing on permissive mode while reading the CSV file, this mode allows us to deal with corrupt records during the parsing. PERMISSIVE sets other fields to null when it meets a corrupted record and stores the malformed string into a new field called _corrupt_record column, this column name can be changed with columnNameOfCorruptRecord  config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec852d7e-8f97-44cf-bbe6-8f43ec94f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = user_schema.add(StructField(name=\"_corrupt_record\", dataType=StringType(), nullable=True))\n",
    "\n",
    "user_csv_df = spark.read.csv(path=\"data/u.user\", \n",
    "                             header=\"false\", \n",
    "                             inferSchema=\"false\", \n",
    "                             mode=\"Permisive\", \n",
    "                             sep=\"|\", \n",
    "                             schema=user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60ab64b2-d64e-4d8c-8481-5737fc298699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+-------+--------------------+\n",
      "| id| age|gender|   occupation|zipcode|     _corrupt_record|\n",
      "+---+----+------+-------------+-------+--------------------+\n",
      "| 17|NULL|     M|   programmer|   6355|17|T|M|programmer...|\n",
      "| 28|  32|     M|       writer|   NULL| 28|32|M|writer|NULL|\n",
      "| 74|  39|     M|    scientist|   NULL|74|39|M|scientist...|\n",
      "|116|NULL|     M|   healthcare|  97232|116|NULL|M|health...|\n",
      "|145|  31|     M|entertainment|   NULL|145|31|M|entertai...|\n",
      "|167|  37|     M|        other|   NULL|167|37|M|other|L9G2B|\n",
      "|201|  27|     M|       writer|   NULL|201|27|M|writer|E...|\n",
      "|213|  33|     M|    executive|   NULL|213|33|M|executiv...|\n",
      "|333|  47|     M|        other|   NULL|333|47|M|other|V0R2M|\n",
      "|458|  47|     M|   technician|   NULL|458|47|M|technici...|\n",
      "|490|  29|     F|       artist|   NULL|490|29|F|artist|V...|\n",
      "|578|  31|     M|administrator|   NULL|578|31|M|administ...|\n",
      "|594|  46|     M|     educator|   NULL|594|46|M|educator...|\n",
      "|599|  22|     F|      student|   NULL|599|22|F|student|...|\n",
      "|634|  39|     M|     engineer|   NULL|634|39|M|engineer...|\n",
      "|709|  21|     M|        other|   NULL|709|21|M|other|N4T1A|\n",
      "|719|  37|     F|        other|   NULL|719|37|F|other|V0R2H|\n",
      "|779|  31|     M|      student|   NULL|779|31|M|student|...|\n",
      "|857|  35|     F|administrator|   NULL|857|35|F|administ...|\n",
      "|901|  38|     M|    executive|   NULL|901|38|M|executiv...|\n",
      "+---+----+------+-------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_csv_df.where(~user_csv_df._corrupt_record.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903395f-720d-4412-971e-f62532f5a98d",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4899e-0527-46fb-8c92-be07c96d60c5",
   "metadata": {},
   "source": [
    "#### Reading the Parquet Data\n",
    "\n",
    "Apache Parquet is a columnar file format that supports the compression and schema enforcement. Apache spark default file format is Parquet and it is widly used in OLAP use cases and modern day data engineering services such as BigQuery, AWS Athena, Hive, snowflake etc.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1126/0*vWHK7tn_sxWl_U6k.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "#### How Parquet stores the data ? \n",
    "Inside the parquet file, data will be stored in 4 parts\n",
    "1. Row Group:\n",
    "    > Horizontal partition of the data \\\n",
    "    > Each row group contains the data for all the columns but only for a subset of rows  \\\n",
    "    > It enables parallel reads  \\\n",
    "2. Column Chunk:\n",
    "    > It holds the values of a single column in a Row Group  \\\n",
    "    > It enables the column level access  \\\n",
    "3. Pages:\n",
    "    > Column chunks are divided into pages  \\\n",
    "    > Data pages contain actual data  \\\n",
    "    > Pages are individually compressed, improving memory usage  \\\n",
    "4. Footer:\n",
    "    > Stored at the end of the Parquet file  \\\n",
    "    > Contains: Column names and data types, Row group info (offset, size), Min/max stats for columns and Compression and encoding info  \\\n",
    "    > Enables: Predicate pushdown, Schema discovery, and Column/row group skipping  \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f7db771-e3fb-44ea-b36e-cc2e2dfdba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parquet file\n",
    "# user_csv_df.repartition(3).write.partitionBy(\"occupation\").mode(\"overwrite\").save(\"data/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9b7f9ad-e81d-46d4-8f50-82e1a6a39034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet file\n",
    "user_parquet_df = spark.read.parquet(\"data/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13e1a0c3-a517-4dcf-96ae-88ef5a771420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+----------+\n",
      "| id|age|gender|zipcode|occupation|\n",
      "+---+---+------+-------+----------+\n",
      "|262| 19|     F|  78264|   student|\n",
      "|140| 30|     F|  32250|   student|\n",
      "|584| 25|     M|  27511|   student|\n",
      "|257| 17|     M|  77005|   student|\n",
      "|372| 25|     F|  66046|   student|\n",
      "|586| 20|     M|  79508|   student|\n",
      "|259| 21|     M|  48823|   student|\n",
      "|632| 18|     M|  55454|   student|\n",
      "| 73| 24|     M|  41850|   student|\n",
      "|361| 22|     M|  44074|   student|\n",
      "+---+---+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_cols = [\"id\", \"age\", \"gender\", \"zipcode\", \"occupation\"]\n",
    "user_parquet_df.where(col(\"occupation\").rlike(\"[^0-9]\")).select(*select_cols).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26d008-181b-414e-b5c1-c0be3cc4df79",
   "metadata": {},
   "source": [
    "#### Reading the JSON data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e64a71-5b2d-42e8-9e83-6f7bda52fe44",
   "metadata": {},
   "source": [
    "Json is nothing but \"java script object Notation\", is a human readable text file format. Normally used for Data exchange.\n",
    "Generally it uses 2 structures Object, Array.\n",
    "1. Objects: Unordered sets of key-value pairs, enclosed in curly braces {}. Each key is a string, and the value can be a primitive data type (string, number, boolean, null), another object, or an arra\n",
    "2. Arrays: Ordered lists of values, enclosed in square brackets []. Values can be any valid JSON data type.\n",
    "\n",
    "Json don't support the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22d34e78-e2c7-427f-a375-a2e91dd205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.option(\"multiline\", True).json(\"data/json_example.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ddaf538-ae54-4b1c-a3b5-ab2691fc6145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------+------------------------+---------+--------+\n",
      "|address             |age|city    |courses                 |isStudent|name    |\n",
      "+--------------------+---+--------+------------------------+---------+--------+\n",
      "|{123 Main St, 10001}|30 |New York|[Math, Physics, History]|false    |John Doe|\n",
      "+--------------------+---+--------+------------------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42f1f834-6074-4d20-813b-0bccb175e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- isStudent: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37257357-eea6-4596-8924-877fe6c1d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---+--------+---------+--------+-------+\n",
      "|street     |zip  |age|city    |isStudent|name    |col    |\n",
      "+-----------+-----+---+--------+---------+--------+-------+\n",
      "|123 Main St|10001|30 |New York|false    |John Doe|Math   |\n",
      "|123 Main St|10001|30 |New York|false    |John Doe|Physics|\n",
      "|123 Main St|10001|30 |New York|false    |John Doe|History|\n",
      "+-----------+-----+---+--------+---------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(col(\"address.street\").alias(\"street\"),\n",
    "               col(\"address.zip\").alias(\"zip\"),\n",
    "               col(\"age\"),col(\"city\"),col(\"isStudent\"), col(\"name\"),\n",
    "               f.explode_outer(\"courses\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cb784e6-bba3-476a-a09d-729d169c22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "new_schema = \"address STRUCT<street: STRING, zip: STRING>, age INT, city STRING, courses ARRAY<STRING>, isstudent BOOLEAN, name STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "94ca8472-6cdf-46c4-9aee-9a7a36a51d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_1 = spark.read.option(\"multiline\", True).text(\"data/json_example.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc96d942-e961-4e54-b15f-2ca83d08010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                   {|\n",
      "|  \"name\": \"John D...|\n",
      "|          \"age\": 30,|\n",
      "|  \"city\": \"New Yo...|\n",
      "|  \"isStudent\": fa...|\n",
      "|        \"address\": {|\n",
      "|    \"street\": \"12...|\n",
      "|      \"zip\": \"10001\"|\n",
      "|                  },|\n",
      "|        \"courses\": [|\n",
      "|             \"Math\",|\n",
      "|          \"Physics\",|\n",
      "|           \"History\"|\n",
      "|                   ]|\n",
      "|                   }|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4878903-f621-47c2-8914-16db43ed135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string_df = json_df_1.select(f.collect_list(\"value\").alias(\"lines\")) \\\n",
    "    .withColumn(\"json_str\", f.concat_ws(\"\", \"lines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6a82156-dd51-4f00-9082-efc2992feb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|lines                                                                                                                                                                                                                             |json_str                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{,   \"name\": \"John Doe\",,   \"age\": 30,,   \"city\": \"New York\",,   \"isStudent\": false,,   \"address\": {,     \"street\": \"123 Main St\",,     \"zip\": \"10001\",   },,   \"courses\": [,     \"Math\",,     \"Physics\",,     \"History\",   ], }]|{  \"name\": \"John Doe\",  \"age\": 30,  \"city\": \"New York\",  \"isStudent\": false,  \"address\": {    \"street\": \"123 Main St\",    \"zip\": \"10001\"  },  \"courses\": [    \"Math\",    \"Physics\",    \"History\"  ]}|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_string_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a1027-5bba-40cf-bd6f-233057841233",
   "metadata": {},
   "source": [
    "#### Write Modes in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da92c6b-f090-429d-b63c-5edb88fbc79a",
   "metadata": {},
   "source": [
    "**Specifies the behavior when data or table already exists.**\n",
    "Options include:\n",
    "1. append: Append contents of this DataFrame to existing data.\n",
    "2. overwrite: Overwrite existing data.\n",
    "3. error or errorifexists: Throw an exception if data already exists.\n",
    "4. ignore: Silently ignore this operation if data already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f44ba8c0-d152-4131-b3af-9e5a1e159d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Default parallelism in spark\n",
    "\n",
    "print(spark.sparkContext.defaultParallelism) # 12\n",
    "print(spark.sparkContext.defaultMinPartitions) #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa257eff-fd22-4248-b82e-c0aabbe2720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------+\n",
      "| id|age|gender|occupation|zipcode|\n",
      "+---+---+------+----------+-------+\n",
      "|  1| 24|     M|technician|  85711|\n",
      "|  2| 53|     F|     other|  94043|\n",
      "+---+---+------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_user_df = user_df.select(\"*\")\n",
    "new_user_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82be8632-086f-4ac6-b241-485479cacf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartitions \n",
    "new_user_df = new_user_df.repartition(5)\n",
    "new_user_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7aa4e31e-57e4-4b85-b264-cfdab1d943c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing the data as parquet with default partitions\n",
    "new_user_df.write.partitionBy(\"occupation\").mode(\"overwrite\").save(\"data/user_parquet.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b7e17-4c30-4bdf-abe6-1f1106f7dfaf",
   "metadata": {},
   "source": [
    "#### deployment modes in Spark\n",
    "##### In Apache Spark, there are two primary deployment modes:\n",
    "1.\t**Client Mode:** In this mode, the Driver Program runs on the machine that submits the application (typically the edge node or master node, outside the cluster). The cluster manager launches the executors on the cluster nodes, but the driver stays on the client machine. \\\n",
    "    a. If the client session (e.g., your terminal or notebook) is terminated, the Spark application will also terminate, since the driver is no longer running. \\\n",
    "    b. This mode is typically used for interactive sessions, debugging, or development purposes. \n",
    "2.\t**Cluster Mode:** In cluster mode, the Driver Program is launched inside the cluster by the cluster manager, alongside the executors. \\\n",
    "    a. This is the recommended approach for production workloads, as it allows the application to continue running independently of the client process that submitted it. \\\n",
    "\tb. Since the driver and executors run within the same cluster, data locality is better and there‚Äôs reduced overhead from network communication between driver and executors. \\\n",
    "\tc. Even if the client disconnects or is terminated, the Spark application continues to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51810103-e287-46d0-81b9-798eb0a436f7",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d497e7b-455e-488a-b995-5b25d851d9f1",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61429bc1-1e1a-4e7e-9271-b8ce7b20cc1b",
   "metadata": {},
   "source": [
    "UDF (User Defined Function): custom function written in Python that you can apply to Spark DataFrame columns when built-in Spark functions (pyspark.sql.functions) are not sufficient.\n",
    "\n",
    "‚úÖ Why Use UDFs?  \\\n",
    "\t‚Ä¢\tTo apply custom Python logic to each row or column.  \\\n",
    "\t‚Ä¢\tWhen there‚Äôs no equivalent Spark function available (e.g., complex text processing, custom regex, external library logic).\n",
    "\n",
    "‚ö†Ô∏è UDF Performance Caveats \\\n",
    "\t‚Ä¢\tSlower than native functions ‚Äî UDFs break Spark‚Äôs optimization (Catalyst engine). \\\n",
    "\t‚Ä¢\tSerialization overhead ‚Äî data is sent between JVM (Spark) and Python (via Py4J).  \\\n",
    "\t‚Ä¢\tHarder to optimize ‚Äî no column pruning or predicate pushdown.\n",
    "\n",
    "üèéÔ∏è Better Alternatives to UDFs (when possible): \\\n",
    "Use native Spark functions from pyspark.sql.functions like: \\\n",
    "\t‚Ä¢\twhen, col, regexp_extract, split, concat_ws \\\n",
    "\t‚Ä¢\texpr(), withColumn(), etc. \n",
    "\n",
    "‚úÖ These are faster and Spark-optimized.\n",
    "\n",
    "üöÄ Pandas UDF (Vectorized UDF) \\\n",
    "If you must use custom Python logic, prefer Pandas UDFs (aka vectorized UDFs), which are faster and optimized for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "11b63c09-31ed-437c-b652-af35a3b90fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.camelCase_v2_fn(word: str)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Way-1\n",
    "@udf(returnType=StringType())\n",
    "def camelCase(word: str):\n",
    "    return word[0].upper() +word[1:]\n",
    "\n",
    "# Way-2\n",
    "def camelCase_v1(word: str):\n",
    "    return word[0].upper() +word[1:]\n",
    "\n",
    "camelCase_v1 = udf(camelCase_v1, StringType())\n",
    "\n",
    "# Way-3\n",
    "def camelCase_v2_fn(word: str):\n",
    "    return word[0].upper() +word[1:]\n",
    "\n",
    "camelCase_v2 = udf(lambda a: camelCase_v2_fn(a), StringType())\n",
    "\n",
    "# Way-4\n",
    "spark.udf.register(\"camelCase_v3\", camelCase_v2_fn, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a108d719-2e03-449d-a5f8-c4a774bdcae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+\n",
      "|occupation|  new_name|new_name_v1|new_name_v2|\n",
      "+----------+----------+-----------+-----------+\n",
      "|technician|Technician| Technician| Technician|\n",
      "|     other|     Other|      Other|      Other|\n",
      "|    writer|    Writer|     Writer|     Writer|\n",
      "|technician|Technician| Technician| Technician|\n",
      "|     other|     Other|      Other|      Other|\n",
      "+----------+----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|occupation|new_name_v3|\n",
      "+----------+-----------+\n",
      "|technician| Technician|\n",
      "|     other|      Other|\n",
      "|    writer|     Writer|\n",
      "|technician| Technician|\n",
      "|     other|      Other|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.select(user_df.occupation, \n",
    "              camelCase(user_df.occupation).alias(\"new_name\"),\n",
    "              camelCase_v1(user_df.occupation).alias(\"new_name_v1\"),\n",
    "              camelCase_v2(user_df.occupation).alias(\"new_name_v2\")).show(5)\n",
    "\n",
    "user_df.createOrReplaceTempView(\"user_df\")\n",
    "\n",
    "spark.sql(\"select occupation, camelCase_v3(occupation) as new_name_v3 from user_df\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a0628-0aef-4364-be0a-335ea4c75cc1",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071fba0-3b90-47f5-8761-9e6816e04594",
   "metadata": {},
   "source": [
    "#### Explain VS toDebugString() / Lineage Topic\n",
    "\n",
    "RDD lineage is the sequence of transformations that defines how an RDD is derived from other RDDs, forming a logical execution graph that Spark uses for fault recovery and optimization. For DataFrames, the transformations are represented as a logical plan and optimized by Spark‚Äôs Catalyst optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "992354b4-e7ba-41aa-8513-145f6d410bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [id#2819,age#2820,gender#2821,occupation#2822,zipcode#2823] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/saimammahi/Documents/Work/Interview/Pyspark-Playground/dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,age:int,gender:string,occupation:string,zipcode:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e3b4fcbd-6740-4822-9281-82ef2095a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(1) MapPartitionsRDD[167] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[166] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[165] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[164] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[163] at javaToPython at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c65987-b8b8-42ce-b465-4301a3ca0284",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16a455-b491-486b-85bd-f03bd8b7613a",
   "metadata": {},
   "source": [
    "#### Optimize the Shuffles in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3076aad-17cc-4972-abcf-10fea377c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_orders_df = spark.read.format(\"parquet\").load(\"data/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5062e628-1de4-438e-b252-39555bcfe656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7820ef70-8385-4242-931c-aff8f6a4bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5942fb98-58b4-4b75-af73-3a13534210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96188a6d-a090-478f-aa49-5ad53f29cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aa545516-5052-409f-9d6b-91dc05d67d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_orders_df_1 = customer_orders_df.select(f.count_distinct(f.col(\"occupation\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bd7fd-1270-463f-a4fb-acd0f48e1302",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eab855-6d70-42a1-bc54-daa8f3880ab2",
   "metadata": {},
   "source": [
    "### Cache VS Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f7129-dba8-4889-963b-1fa4bbec5ef9",
   "metadata": {},
   "source": [
    "- **Caching** and **persisting** are techniques to improve performance for **iterative or interactive Spark workloads** by storing intermediate results.\n",
    "\n",
    "- These methods avoid recomputing expensive transformations across multiple actions.\n",
    "\n",
    "#### 1. Default Behavior of `cache()`\n",
    "\n",
    "- For **RDDs**:\n",
    "  - `cache()` uses the default storage level: **`MEMORY_ONLY`**.\n",
    "  \n",
    "- For **DataFrames/Datasets**:\n",
    "  - `cache()` defaults to **`MEMORY_AND_DISK`**.\n",
    "\n",
    "In both cases, `cache()` is a shorthand for `persist()` using the default level.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Using `persist()` with Custom Storage Levels\n",
    "\n",
    "- `persist()` allows specifying a storage level explicitly.\n",
    "- Available levels include:\n",
    "  - `MEMORY_ONLY`\n",
    "  - `MEMORY_ONLY_SER` (serialized in memory): Only for Java and Scala, in Pyspark we can't use this\n",
    "  - `MEMORY_AND_DISK`\n",
    "  - `MEMORY_AND_DISK_SER`: Only for Java and Scala, in Pyspark we can't use this\n",
    "  - `DISK_ONLY`\n",
    "  - And replication variants like `MEMORY_ONLY_2` (replicated twice)\n",
    "\n",
    "Use `persist()` when you need more flexibility over memory/disk usage or serialization.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Storage Levels: Trade-offs\n",
    "\n",
    "| Storage Level         | Memory Use | CPU Use         | In Memory | On Disk | Serialized | Recompute on Failure |\n",
    "|-----------------------|------------|------------------|-----------|---------|------------|----------------------|\n",
    "| `MEMORY_ONLY`         | High       | Low              | Yes       | No      | No         | Yes                  |\n",
    "| `MEMORY_ONLY_SER`     | Low        | High (serialize) | Yes       | No      | Yes        | Yes                  |\n",
    "| `MEMORY_AND_DISK`     | High       | Medium           | Some      | Some    | No         | No                   |\n",
    "| `MEMORY_AND_DISK_SER` | Low        | High             | Some      | Some    | Yes        | No                   |\n",
    "| `DISK_ONLY`           | Low        | High (I/O)       | No        | Yes     | Yes        | No                   |\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Usage Guidelines & Behavior\n",
    "\n",
    "- **Lazy evaluation**: `cache()` and `persist()` do not immediately store data‚Äîit happens when an **action** is performed (e.g., `.count()`, `.show()`).\n",
    "- **Viewing storage**: Use the Spark UI under the ‚ÄúStorage‚Äù tab to monitor cached datasets.\n",
    "- **Removing cached data**: Use `.unpersist()` to free up memory when a DataFrame or RDD is no longer needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eaf15c0a-ebe7-4223-95ff-7c2deaca050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.read.parquet('data/user.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9e436e86-18f9-42a0-b23f-54eedb63761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+---------------+----------+\n",
      "|id |age|gender|zipcode|_corrupt_record|occupation|\n",
      "+---+---+------+-------+---------------+----------+\n",
      "|262|19 |F     |78264  |NULL           |student   |\n",
      "|140|30 |F     |32250  |NULL           |student   |\n",
      "+---+---+------+-------+---------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88bcdd98-4e27-44a3-9c22-1b8705360146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the df and it stores the data in MEMORY_AND_DISK\n",
    "new_user_df = user_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "58f80f6e-42fe-45d1-b7d1-7970dbe845fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "new_user_df_2 = new_user_df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "241a2e3f-2209-497d-af21-3976ef9046f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, age: int, gender: string, zipcode: int, _corrupt_record: string, occupation: string]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "133177f4-c9c6-4671-9acf-33924ad6a388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, age: int, gender: string, zipcode: int, _corrupt_record: string, occupation: string]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpersist the cache df\n",
    "new_user_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "16a068b2-457d-46cf-baca-68cb906278c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, age: int, gender: string, zipcode: int, _corrupt_record: string, occupation: string]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpersist the persisting df\n",
    "new_user_df_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7228152a-16b2-48f3-902c-5afb6abf403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the cache on cluster level\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d96233-1b3f-4ad3-9acf-4ef0d1d54de7",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d0c9a-ab49-42b7-9a0e-d64e99f8a85c",
   "metadata": {},
   "source": [
    "### Broadcast Variable and Accumlators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af996b6-2523-41e1-9483-bc7b2d3db732",
   "metadata": {},
   "source": [
    "#### What is a Broadcast Variable?\n",
    " - **A broadcast variable allows you to share a read-only variable across all Spark executors efficiently**.\n",
    " - Useful when you have a large lookup table or reference dataset that needs to be used in multiple tasks.\n",
    " - Spark sends the variable once to each node rather than with every task, saving network I/O.\n",
    "\n",
    "When to Use:\n",
    "- Joining a small dataset with a large RDD/DataFrame (map-side join).\n",
    "- Any read-only data that is reused across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee87a6df-1fcf-47e2-ac48-e29cefd9cb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|   name|country|    continent|\n",
      "+-------+-------+-------------+\n",
      "|  Alice|     US|North America|\n",
      "|    Bob|     UK|       Europe|\n",
      "|Charlie|     US|North America|\n",
      "|  David|     CA|North America|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"BroadcastExample\").getOrCreate()\n",
    "\n",
    "# Example DataFrame\n",
    "data = [(\"Alice\", \"US\"), (\"Bob\", \"UK\"), (\"Charlie\", \"US\"), (\"David\", \"CA\")]\n",
    "df = spark.createDataFrame(data, [\"name\", \"country\"])\n",
    "\n",
    "# Small lookup table: country -> continent\n",
    "country_continent = {\"US\": \"North America\", \"UK\": \"Europe\", \"CA\": \"North America\"}\n",
    "\n",
    "# Broadcast the lookup table\n",
    "broadcast_var = spark.sparkContext.broadcast(country_continent)\n",
    "\n",
    "# Define UDF using broadcast variable\n",
    "def get_continent(country):\n",
    "    return broadcast_var.value.get(country, \"Unknown\")\n",
    "\n",
    "get_continent_udf = udf(get_continent)\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "df.withColumn(\"continent\", get_continent_udf(col(\"country\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb228f9-0e13-4323-be94-961e23aabafd",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d61795-dd62-4f6d-a753-5924fe41a959",
   "metadata": {},
   "source": [
    "# üîÄ Spark Join Strategies: Shuffle Hash Join vs Sort Merge Join vs Broadcast Join\n",
    "\n",
    "In Spark, when performing **joins** between two DataFrames (or RDDs), the **execution strategy** chosen by the Catalyst optimizer can have a big impact on performance.  \n",
    "Here are the three main join strategies:\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Shuffle Hash Join (SHJ)\n",
    "\n",
    "### ‚úÖ How it Works\n",
    "- Both input DataFrames are **shuffled** by the join key into partitions.\n",
    "- A **hash table** is built on the smaller side within each partition.\n",
    "- The other side scans and probes the hash table to find matches.\n",
    "\n",
    "### üìå Characteristics\n",
    "- Best when one side of the join is **small enough to fit in memory**.\n",
    "- **Hash-based matching** makes it fast for **equality joins** (`=`, `IN`).\n",
    "- Requires **shuffling** both datasets.\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "- Memory intensive (hash table must fit in memory).\n",
    "- Not efficient for **large-to-large joins**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Sort Merge Join (SMJ)\n",
    "\n",
    "### ‚úÖ How it Works\n",
    "1. Both input DataFrames are **shuffled** by the join key.\n",
    "2. Within each partition, data is **sorted**.\n",
    "3. A **merge algorithm** (like merge-sort) scans both sorted partitions to find matches.\n",
    "\n",
    "### üìå Characteristics\n",
    "- Scales better for **very large datasets**.\n",
    "- Works with **non-equality joins** (`<`, `<=`, `>`, etc.), unlike hash join.\n",
    "- Efficient if data is **already sorted** or partitioned.\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "- Sorting step is expensive (high CPU cost).\n",
    "- Requires large shuffle operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Broadcast Hash Join (BHJ)\n",
    "\n",
    "### ‚úÖ How it Works\n",
    "- Spark **broadcasts the smaller DataFrame** to all executors.\n",
    "- Each executor keeps a **copy of the small DataFrame** in memory.\n",
    "- The join runs as a **map-side join**: no shuffle required.\n",
    "\n",
    "### üìå Characteristics\n",
    "- Extremely fast when one table is **small (e.g., < 10MB by default)**.\n",
    "- Avoids expensive shuffles.\n",
    "- Great for **star schema joins** (fact table + dimension table).\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "- Only works if one dataset is **small enough to fit in executor memory**.\n",
    "- Can cause **OOM errors** if broadcast size is too big.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë When to Use Which?\n",
    "\n",
    "| Join Type        | Best Scenario                                    | Avoid When                                    |\n",
    "|------------------|--------------------------------------------------|-----------------------------------------------|\n",
    "| **Shuffle Hash** | One dataset is moderately small but not tiny      | Both datasets are very large                  |\n",
    "| **Sort Merge**   | Both datasets are very large, non-equality joins | One dataset can fit in memory (use Broadcast) |\n",
    "| **Broadcast**    | One dataset is very small (< 10MB)               | Dataset is large ‚Üí risk of OOM                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7daf7afb-a647-4008-86aa-c1f6c372132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- LogicalRDD [id#4556L, val1#4557], false\n",
      "+- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, val1: string, val2: string\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L)\n",
      "   :- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L)\n",
      "   :- Filter isnotnull(id#4556L)\n",
      "   :  +- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- Filter isnotnull(id#4560L)\n",
      "      +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#4556L, val1#4557, val2#4561]\n",
      "   +- SortMergeJoin [id#4556L], [id#4560L], Inner\n",
      "      :- Sort [id#4556L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#4556L, 200), ENSURE_REQUIREMENTS, [plan_id=4447]\n",
      "      :     +- Filter isnotnull(id#4556L)\n",
      "      :        +- Scan ExistingRDD[id#4556L,val1#4557]\n",
      "      +- Sort [id#4560L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#4560L, 200), ENSURE_REQUIREMENTS, [plan_id=4448]\n",
      "            +- Filter isnotnull(id#4560L)\n",
      "               +- Scan ExistingRDD[id#4560L,val2#4561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ‚ö° Spark Join Strategy Examples (PySpark)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinStrategies\").getOrCreate()\n",
    "\n",
    "# Example DataFrames\n",
    "data1 = [(1, \"A\"), (2, \"B\"), (3, \"C\")]\n",
    "data2 = [(1, \"X\"), (2, \"Y\"), (3, \"Z\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"val1\"])\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"val2\"])\n",
    "result = df1.join(df2, \"id\", \"inner\")\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8e1547f9-f8ad-4bfd-90e7-4ec93657f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- LogicalRDD [id#4556L, val1#4557], false\n",
      "+- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, val1: string, val2: string\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L)\n",
      "   :- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L)\n",
      "   :- Filter isnotnull(id#4556L)\n",
      "   :  +- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- Filter isnotnull(id#4560L)\n",
      "      +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#4556L, val1#4557, val2#4561]\n",
      "   +- SortMergeJoin [id#4556L], [id#4560L], Inner\n",
      "      :- Sort [id#4556L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(id#4556L, 200), ENSURE_REQUIREMENTS, [plan_id=4478]\n",
      "      :     +- Filter isnotnull(id#4556L)\n",
      "      :        +- Scan ExistingRDD[id#4556L,val1#4557]\n",
      "      +- Sort [id#4560L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(id#4560L, 200), ENSURE_REQUIREMENTS, [plan_id=4479]\n",
      "            +- Filter isnotnull(id#4560L)\n",
      "               +- Scan ExistingRDD[id#4560L,val2#4561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Force Sort-Merge by disabling broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100000000)\n",
    "\n",
    "result = df1.join(df2, \"id\", \"inner\")\n",
    "result.explain(True)  # See physical plan ‚Üí SortMergeJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63034786-fa3d-40d1-9025-3aafdbdead7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- LogicalRDD [id#4556L, val1#4557], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, val1: string, val2: string\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L)\n",
      "   :- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#4556L, val1#4557, val2#4561]\n",
      "+- Join Inner, (id#4556L = id#4560L), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(id#4556L)\n",
      "   :  +- LogicalRDD [id#4556L, val1#4557], false\n",
      "   +- Filter isnotnull(id#4560L)\n",
      "      +- LogicalRDD [id#4560L, val2#4561], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#4556L, val1#4557, val2#4561]\n",
      "   +- BroadcastHashJoin [id#4556L], [id#4560L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#4556L)\n",
      "      :  +- Scan ExistingRDD[id#4556L,val1#4557]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=4508]\n",
      "         +- Filter isnotnull(id#4560L)\n",
      "            +- Scan ExistingRDD[id#4560L,val2#4561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = df1.join(broadcast(df2), \"id\", \"inner\")\n",
    "result.explain(True)  # See physical plan ‚Üí BroadcastHashJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31909878-27a3-47c1-8305-3990689905a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
